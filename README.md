# Zipco Foods Orchestration Using Apache Airflow

### Leveraging the Power of Apache Airflow for orchestration the ETL processes, scheduling jobs efficiently and monitoring the workflow of data through various stages of the pipeline

<img width="1000" height="500" alt="images" src="https://github.com/user-attachments/assets/5c9e8768-b6fb-4f7a-bf3a-3a672f93c91c" />

_Disclaimer⚠️: All datasets and reports do not contain real proprietary, confidential, or sensitive information from any organization, institution, or individual. It is worth noting that dummy data were utilized to demonstrate my capabilities of using Python, Apache Airflow and Microsoft Azure to orchestrate ETL process and store it on a cloud platform.
_
## INTRODUCTION

Zipco Foods is a vibrant and growing business that specializes in the sales of pizzas and cakes. As a key player in the fast-casual dining industry, Zipco Foods operates numerous outlets across the country, serving a wide variety of pizzas and cakes that cater to local tastes and preferences. With a strong commitment to customer satisfaction and quality service, Zipco Foods aims to leverage advanced data engineering solutions to enhance operational efficiency, improve product offerings, and ultimately boost profitability.

## PROBLEM STATEMENT

Zipco Foods generates a significant amount of sales data daily, which is currently underutilized due to inefficient data handling and analysis processes. 

The primary challenge is the disparate nature of data collection and storage, with critical sales and inventory information scattered across multiple CSV files without a unified system for aggregation and analysis. This fragmentation leads to operational inefficiencies, including delays in data access, difficulty in obtaining real-time insights, and challenges in maintaining data integrity and accuracy.

## OBJECTIVES

- Implement a streamlined ETL (Extract, Transform, Load) pipeline to automate data processing and ensure data consistency.
- Design a database schema that supports efficient data retrieval and scalability while adhering to 2NF/3NF normalization standards.
- Develop a system for real-time data analytics to aid in decision-making processes.
- Ensure robust data governance and compliance through effective version control and data orchestration.

## BENEFITS

- Enhanced decision-making capabilities through real-time, accurate data analytics.
- Improved operational efficiency and reduced manual labor by automating data processes.
- Greater scalability and flexibility in data management, accommodating future business growth.
- Strengthened data integrity and reliability, ensuring high-quality information for strategic planning.

## TECH STACK

- Python: Utilized for scripting the ETL processes, data cleaning, transformation, and analysis tasks due to its powerful libraries like pandas and NumPy.
- SQL: Employed for querying, updating, and managing the database stored in Azure, ensuring efficient data manipulation and retrieval.
- Azure Blob Storage: Chosen for its scalability and reliability, serving as the centralized data repository for storing processed data.
- Github: Used for version control, allowing for collaborative development and maintenance of the ETL scripts and other project documents.
- Apache Airflow: Orchestrates the ETL processes, scheduling jobs efficiently and monitoring the workflow of data through various stages of the pipeline.
